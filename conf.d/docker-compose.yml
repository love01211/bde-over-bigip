version: "3"

services:
  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1
  #   container_name: ELASTICSEARCH
  #   ports:
  #     - 9200:9200
  #   environment:
  #     - discovery.type=single-node
  #   volumes:
  #    - ${HOMEDIR}/data/elasticsearch:/usr/share/elasticsearch/data
  #   # entrypoint:
  #   #   - /usr/local/bin/docker-entrypoint.sh
  #   #   - eswrapper

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1
    container_name: ES01
    ports:
      - 9200:9200
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
     - ${HOMEDIR}/data/es01:/usr/share/elasticsearch/data
    
  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1
    container_name: ES02
    environment:
      - node.name=es02
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
     - ${HOMEDIR}/data/es02:/usr/share/elasticsearch/data

  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1
    container_name: ES03
    environment:
      - node.name=es03
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
     - ${HOMEDIR}/data/es03:/usr/share/elasticsearch/data

  fluentd:
    build: ${HOMEDIR}/docker/fluentd
    image: f5networks/fluentd:latest
    container_name: FLUENTD
    ports:
      # - 24224:24224
      # - 24224:24224/udp
      - 20001:20001/udp
      # - 8081-8089:8081-8089
      # - 8085:8085
    volumes:
      - ${HOMEDIR}/conf.d/fluentd.conf:/etc/td-agent/td-agent.conf
      - ${HOMEDIR}/conf.d/fluentd:/etc/td-agent/fluentd
      - ${HOMEDIR}/scripts/cmds-in-fluentd:/root/cmds-in-fluentd
      - ${HOMEDIR}/conf.d/fluentd-crontab:/etc/crontab
      - ${HOMEDIR}/data/fluentd:/var/log/td-agent/buffer
    links:
      - es01:elasticsearch
    depends_on:
      - es01
      - kibana
      # - kafka

  kibana:
    image: docker.elastic.co/kibana/kibana:7.4.1
    container_name: KIBANA
    links:
      - es01:elasticsearch
    depends_on:
      - es01
    ports:
      - 5601:5601
    volumes:
      - ${HOMEDIR}/conf.d/kibana.yml:/usr/share/kibana/config/kibana.yml
      - ${HOMEDIR}/data/kibana:/usr/share/kibana/data
    # entrypoint:
    #   - /usr/local/bin/dumb-init
    #   - --
    #   - /usr/local/bin/kibana-docker

  ctrlbox:
    build: ${HOMEDIR}/docker/ctrlbox
    image: f5networks/ctrlbox:latest
    container_name: CTRLBOX
    ports:
      - 8000:80
    links:
      - es01:elasticsearch
    depends_on:
      - nginx
      - kibana
      - es01
      - fluentd
      # - kafka
      # - logstash
    env_file:
      - ${HOMEDIR}/conf.d/.setup.rc
    volumes:
      - ${HOMEDIR}:/root/workdir
      - ${HOMEDIR}/conf.d/ctrlbox-crontab:/etc/crontab
      # - ${HOMEDIR}/scripts:/root/scripts
      # - ${HOMEDIR}/conf.d/setup.rc:/root/setup.rc # temp
      # - ${HOMEDIR}/conf.d/kibana-exports:/root/kibana-exports
    # entrypoint:
    #   - /bin/sh
    #   - -c
    #   - while true; do sleep 1; done

  # zookeeper:
  #   image: wurstmeister/zookeeper:latest
  #   expose:
  #   - "2181"

  # kafka:
  #   image: wurstmeister/kafka:2.12-2.4.0
  #   depends_on:
  #   - zookeeper
  #   ports:
  #   - "9092:9092"
  #   environment:
  #     # KAFKA_BROKER_ID: 1
  #     # BROKER_ID_COMMAND: "printf '%d' $((0x$HOSTNAME % 10000))"
  #     HOSTNAME_COMMAND: "hostname"
  #     KAFKA_ADVERTISED_HOST_NAME: _{HOSTNAME_COMMAND}
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
  #     KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     # KAFKA_CREATE_TOPICS: "messages:1:1,test:1:1:compact"
  #     # KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
  #     # KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
  #     # KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

  # zookeeper:
  #   image: bitnami/zookeeper:3
  #   container_name: ZOOKEEPER
  #   ports:
  #     - 2181:2181
  #   # volumes:
  #   #   - ${HOMEDIR}/data/zookeeper:/bitnami
  #   environment:
  #     - ALLOW_ANONYMOUS_LOGIN=yes
    
  # kafka:
  #   image: bitnami/kafka:2
  #   container_name: KAFKA
  #   ports:
  #     - 9092:9092
  #     - 9093:9093
  #   volumes:
  #     - ${HOMEDIR}/data/kafka:/bitnami/kafka
  #   environment:
  #     - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #     - ALLOW_PLAINTEXT_LISTENER=yes
  #     - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9093,EXTERNAL://localhost:9092
  #     - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
  #     - KAFKA_CFG_LISTENERS=INTERNAL://kafka:9093,EXTERNAL://localhost:9092
  #     - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
  #     # fix error of fluentd: error_class=Kafka::MessageSizeTooLarge error="Kafka::MessageSizeTooLarge"
  #     - KAFKA_CFG_MESSAGE_MAX_BYTES=1073741824 # 1G
  #     # data retention settings
  #     - KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS=10000 # 10s
  #     - KAFKA_CFG_LOG_RETENTION_HOURS=1 # 1hour
  #     - KAFKA_CFG_LOG_RETENTION_BYTES=1073741824  #1GB
  #   depends_on:
  #     - zookeeper

  # logstash:
  #   build: ${HOMEDIR}/docker/logstash
  #   image: f5networks/logstash:latest
  #   container_name: LOGSTASH
  #   links:
  #     - es01:elasticsearch
  #   depends_on:
  #     - es01
  #   ports:
  #     # - 4560:4560 # for logstash debug test.
  #     # - 5000:5000
  #     - 9600:9600
  #   volumes:
  #     - ${HOMEDIR}/conf.d/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
  #     - ${HOMEDIR}/conf.d/logstash.pipelines:/usr/share/logstash/config/pipelines.yml
  #   entrypoint:
  #     - /usr/local/bin/docker-entrypoint
  #     - -r    # Make logstash reload automatically when conf. changes.
    
    
  nginx:
    image: nginx:latest
    container_name: NGINX
    ports:
      - 20000:20000/udp
    volumes:
      - ${HOMEDIR}/conf.d/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - fluentd
