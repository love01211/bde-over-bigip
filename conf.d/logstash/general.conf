input {
    kafka {
        type => "kafka"
        bootstrap_servers => "kafka:9093"
        topics => [
            # "ltm-fluentd"  # need to be same as index prefix.
            "general-topic"
        ]
        decorate_events => "true"
    }
}

filter {
    json {
        # 从字符串（同样也是 json 格式）的 message 字段生成 json，字符串其他内容丢弃
        source => "message"
    }
}



filter {
    # []用来访问 json 中的字段
    if ("" in [mesg_hex]) {
        if [mesg_hex] !~ /0$/ { # 1/16
        # if [mesg_hex] !~ /[0-7]$/ { # half
        # if [mesg_hex] !~ /[0-9a-z]$/ { # all
            drop {}
        }
    }
}


filter {
    grok {
        patterns_dir => ["/usr/share/logstash/patterns_dir"]
        match => [
            # <13>Apr 14 00:38:46 l4 2020-04-14 00:38:46.115571 (19233) [E]: [nginx1.ljx.test]: FQDN does not exist. DNS query returns NXDOMAIN
            "data", "(<.*>)?.* (?<host>\w+) %{SYSLOG_TIME:syslogtime}\.\d+ \(%{NUMBER:pid}\) \[(?<log_level>\w)\]: \[%{URL:dns_query}\]: (?<dns_result>.*)",

            # <133>Apr 13 16:44:50 l4.gf.com notice mcpd[19290]: 01070638:5: Pool /Common/nginx member /Common/10.1.10.146:80 monitor status down. [ /Common/tcp: down; last error: /Common/tcp: Unable to connect. @2020/04/13 16:44:50.  ]  [ was up for 0hr:3mins:56sec ]
            "data", "<.*>(?<syslogtime>\w{3} \d{2} \d{2}:\d{2}:\d{2}) %{HOSTNAME:host} (?<log_level>\w+) (?<process_name>\w+)\[%{NUMBER:pid}\]: %{NUMBER:status_code}:.* Pool (?<pool_path>\S+) member (?<member_path>\S+) monitor status (?<monitor_status>(\w+)). \[ (?<monitor_name>\S+): .*; last error: (?<last_error>.*) \]  \[ was \w+ for (?<was_duration>.*) \]",
            
            # 2020-03-26 11:03:11.770163 (6597) [D]: Received DNS results for [www.bbb.com] family [1]
            "data", ".*%{SYSLOG_TIME:syslogtime}.\d+ \(%{NUMBER:pid}\) \[(?<log_level>\w)\]: Received DNS results for \[%{URL:dns_query}\] family \[(?<family>\w+)\].*",
            
            # 2020-03-26 11:03:11.770192 (6597) [d]: (dns_resolved): address 192.168.20.251 TTL: 30
            "data", ".*%{SYSLOG_TIME:syslogtime}.\d+ \(%{NUMBER:pid}\) \[(?<log_level>\w)\]: \(dns_resolved\): address %{IPV4:ip_addr} TTL: (?<ttl>\d+).*",

            # 2020-03-26 11:23:27.319982 (6597) [I]: FqdnProcessor [www.aaa.com] timed out. Schedule next in 5 seconds.
            "data", ".*%{SYSLOG_TIME:syslogtime}.\d+ \(%{NUMBER:pid}\) \[(?<log_level>\w)\]: FqdnProcessor \[%{URL:dns_query}\] (?<reason>.*).*",
            "data", "(?<unresolved>.*)"
        ]
    }
}

filter {
    date {
        match => [
            "syslogtime", "yyyy-MM-dd HH:mm:ss", "MMM dd HH:mm:ss"
        ]
        target => "timestamp"
    }
}

filter {
    mutate {
        # 带@的是logstash默认的 field
        remove_field => [
            # "@timestamp", # '%{+YYYY.MM.dd.HH}' will use @timestamp, so remove it here -- after add_field
            "message",   # comment this line for debug when looking into the original message
            "@version"
        ]
         add_field => {
             stdout => "OK"
         }
    }
}

output{
    if [stdout] == 'OK' {
        stdout { codec => rubydebug }
    }
    if [type] == "kafka" {
        elasticsearch { 
            hosts => ["elasticsearch:9200"]
            # index => "%{[index]}"
            ilm_rollover_alias => "general"
            ilm_pattern => "000001"
            ilm_policy => "general-ilm"
            template => "/usr/share/logstash/general-template.tmpl"
            template_name => "general-template"
        }
    }
}

